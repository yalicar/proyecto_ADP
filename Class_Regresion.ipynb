{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar data set\n",
    "data_array = np.load ('data_proyecto/proyecto_data/proyecto_training_data.npy')\n",
    "#data = pd.DataFrame (data_array,columns=[\"SalePrice\",\"OverallQual\",\"stFlrSF\",\"TotRmsAbvGrd\",\"YearBuilt\",\"LotFrontage\"])\n",
    "#data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables seleccionadas\n",
    "stFlrSF =data_array[:,2]\n",
    "OverallQual=data_array[:,1]\n",
    "SalesPrice =data_array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regresion():\n",
    "    \n",
    "    def __init__(self,X_train,X_test,y_train,y_test,α,epochs):\n",
    "        self.X_train=X_train\n",
    "        self.y_train=y_train\n",
    "        self.X_test=X_test\n",
    "        self.y_test=y_test\n",
    "        self.α=α\n",
    "        self.epochs=epochs\n",
    "        \n",
    "    def funCosto(self,betas):\n",
    "        x_b=np.hstack([self.X_train,np.ones_like(self.X_train)])  # Matriz compuesta\n",
    "        return np.sum(np.square((x_b.dot(betas) - self.y_train))) / (2 * len(self.X_train))  # Calculo del error\n",
    "    \n",
    "    def funGrad(self, theta):\n",
    "        m=len(self.X_train)\n",
    "        x_b=np.hstack([self.X_train,np.ones_like(self.X_train)])\n",
    "        b0_grad=np.sum((x_b.dot(theta) - self.y_train)) / (m)\n",
    "        b1_grad=np.sum(np.transpose((x_b.dot(theta) - self.y_train)) * (np.transpose(np.transpose(x_b)[1]))) / (m)\n",
    "        return b1_grad,b0_grad\n",
    "    \n",
    "    def DescGrad(self):\n",
    "        hist_cost = []                      # Lista vacia para almacenar el error en cada iteracion\n",
    "        hist_grad = []                      # Lista vacia para almacenar el gradiente del error\n",
    "        n_iter = []                         # Lista vacia para almacenar el numero de iteracion correcpondiente al error\n",
    "        theta = [[0],[0]]            # Inicializacion de los parametros b0 y b1 del modelo\n",
    "        x_b=np.hstack([self.X_train,np.ones_like(self.X_train)])  # Matriz compuesta de dos columnas\n",
    "        hist_theta=[]                       # Lista vacia para almacenar todos los betas\n",
    "        m=len(self.X_train)                            # Longitud del arreglo\n",
    "        for _ in range(self.epochs):\n",
    "            #gradiente del error para b0\n",
    "            theta[0][0] = theta[0][0] - α * np.sum((x_b.dot(theta) - self.y_train)) / (m)\n",
    "            # Gradiente del error para b1\n",
    "            theta[1][0] = theta[1][0] - self.α * np.sum(np.transpose((x_b.dot(theta) - self.y_train)) * (np.transpose(np.transpose(x_b)[1]))) / (m)\n",
    "        \n",
    "            hist_theta.append([theta[0][0],theta[1][0]])\n",
    "            hist_cost.append(funCosto(theta))\n",
    "            hist_grad.append(funGrad(theta))\n",
    "            n_iter.append(_)\n",
    "        return theta,np.array(n_iter),np.array(hist_cost),np.array(hist_theta),np.array(hist_grad)\n",
    "    \n",
    "    def prediccion(self,x,thetas_hist):\n",
    "        x=np.array(x).reshape(-1,1)\n",
    "        thetas_hist=np.array(thetas_hist).reshape(-1,1)\n",
    "        x=np.hstack([x,np.ones_like(x)])\n",
    "        pred=x.dot(thetas_hist)\n",
    "        return pred\n",
    "        \n",
    "    @classmethod \n",
    "    def parametros(cls,X,Y,α,epoch):\n",
    "        X_train, X_test, y_train, y_test =train_test_split(X,Y,test_size = 0.20,shuffle=False)\n",
    "        \n",
    "        X_train=np.reshape(X_train,(-1,1)) /1000\n",
    "        y_train=np.reshape(y_train,(-1,1))/1000 # Vector dividido entre 1000 ya que las variables se desvordan por ser valores muy grandes\n",
    "        X_test=np.reshape(X_test,(-1,1))/1000\n",
    "        y_test=np.reshape(y_test,(-1,1))/1000\n",
    "        return cls(X_train,X_test,y_train,y_test,α,epoch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl=Regresion.parametros(stFlrSF,SalesPrice,0.7,8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.6429109589041097, -0.6429109589041097)\n"
     ]
    }
   ],
   "source": [
    "print(rl.funGrad([[134],[25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
